{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Image Feature Extraction and Matching\n",
    "\n",
    "This is Project 1 for [UW CSE P576 Computer Vision](https://courses.cs.washington.edu/courses/csep576/18sp). \n",
    "\n",
    "**Getting Started:** To get started, **[download the source files here (Projects 1 and 2)](https://courses.cs.washington.edu/courses/csep576/18sp/projects/project12/project12.zip \"Project 1 and 2 Source Files\")**. To run the project locally you will need IPython/Jupyter installed, e.g., see instructions at http://jupyter.org/install.html. Launch Jupyter and open `Project1.ipynb`. Alternatively, you can import the standalone version of the notebook into [Colaboratory](https://colab.research.google.com \"Colab\") and run it without installing anything. Use File->Upload Notebook in Colab and open the notebook in `standalone/Project1s.ipynb`.\n",
    "\n",
    "**This project:** In this project you will build an image feature matcher, starting with simple convolution operations and working through interest point detection and descriptor extraction. Once you have a basic feature matcher working, try out some improvements and document your results. If youâ€™re not already familiar with python/numpy, it is recommended to do an introduction such as: http://cs231n.github.io/python-numpy-tutorial. \n",
    "\n",
    "**What to turn in:** Turn in a pdf or static html copy of your completed ipynb notebook as well as the source .ipynb and any source .py files that you modified. Clearly describe any enhancements or experiments you tried in your ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "from time import time\n",
    "import types\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import im_util\n",
    "import interest_point\n",
    "\n",
    "%matplotlib inline\n",
    "# edit this line to change the figure size\n",
    "plt.rcParams['figure.figsize'] = (16.0, 10.0)\n",
    "# force auto-reload of import modules before running code \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Image Filtering [25%]\n",
    "\n",
    "Start by writing code to perform convolution in 1D. Open `im_util.py` and edit the function `convolve_1d`. You should use only basic numpy array operations and loops. Don't worry about efficiency for now. You should see small errors compared to the reference numpy version.\n",
    "\n",
    "Note that convolution and correlation are the same under a simple manipulation of the kernel (what is it?). For what kernels are convolution and correlation results identical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test of convolve_1d\n",
    "\"\"\"\n",
    "print('[ Test convolve_1d ]')\n",
    "x = (np.random.rand(20)>0.8).astype(np.float32)\n",
    "k = np.array([1,3,1])\n",
    "y1 = im_util.convolve_1d(x, k)\n",
    "y2 = np.convolve(x, k, 'same')\n",
    "y3 = np.correlate(x, k, 'same')\n",
    "print(' convolve error = ', np.sum((y1-y2)**2))\n",
    "print(' correlate error = ', np.sum((y1-y3)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now convolve a 2D image with a 1D kernel. Before you begin, get some image data by running `get_data.sh` in the `data` directory. Then complete the function `convolve_rows` in `im_util.py` by convolving every row of the image by the kernel. Run the code below and check that the image output is sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test of convolve_image\n",
    "\"\"\"\n",
    "image_filename='data/test/100-0038_img.jpg'\n",
    "\n",
    "print('[ Test convolve_image ]')\n",
    "im = im_util.image_open(image_filename)\n",
    "k = np.array([1,2,3,4,5,6,5,4,3,2,1])\n",
    "print(' convolve_rows')\n",
    "t0=time()\n",
    "im1 = im_util.convolve_rows(im, k)\n",
    "t1=time()\n",
    "print(' % .2f secs' % (t1-t0))\n",
    "print(' scipy convolve')\n",
    "t0=time()\n",
    "im2 = im_util.convolve(im, np.expand_dims(k,0))\n",
    "t1=time()\n",
    "print(' % .2f secs' % (t1-t0))\n",
    "print(' convolve_image error =', np.sum((im1-im2)**2))\n",
    "\n",
    "# optionally plot images for debugging\n",
    "#im1_norm=im_util.normalise_01(im1)\n",
    "#im2_norm=im_util.normalise_01(im2)\n",
    "#ax1,ax2=im_util.plot_two_images(im1_norm, im2_norm)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will probably find that the scipy convolve runs much faster than your version. To speed things up you can use this version (`im_util.convolve`) for all subsequent experiments. Note that this performs a general 2D convolution with a 2D kernel as input. \n",
    "\n",
    "Now write code to perform Gaussian blurring. First implement the function `gauss_kernel` to compute a 1D Gaussian kernel. Then complete `convolve_gaussian` to perform a separable convolution with this kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gaussian blurring test\n",
    "\"\"\"\n",
    "print('[ Test convolve_gaussian ]')\n",
    "\n",
    "sigma=4.0\n",
    "k=im_util.gauss_kernel(sigma)\n",
    "print(' gauss kernel = ')\n",
    "print(k)\n",
    "\n",
    "t0=time()\n",
    "im1 = im_util.convolve_gaussian(im, sigma)\n",
    "t1=time()\n",
    "print(' % .2f secs' % (t1-t0))\n",
    "\n",
    "ax1,ax2=im_util.plot_two_images(im, im1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write code to compute horizontal and vertical gradients in the function `compute_gradients`. Use an explicit kernel that is convolved in each direction (i.e., do not use a built-in function such as `numpy.gradient`). Run the code below and check that the outputs look sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gradient computation test\n",
    "\"\"\"\n",
    "print('[ Test gradient computation ]')\n",
    "img = np.mean(im,2,keepdims=True)\n",
    "Ix,Iy = im_util.compute_gradients(img)\n",
    "\n",
    "# copy greyvalue to RGB channels\n",
    "Ix_out = im_util.grey_to_rgb(im_util.normalise_01(Ix))\n",
    "Iy_out = im_util.grey_to_rgb(im_util.normalise_01(Iy))\n",
    "\n",
    "im_util.plot_two_images(Ix_out, Iy_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interest Point Extractor [25%]\n",
    "\n",
    "Now you will use these convolution functions to implement a corner or interest point detector. Choose a well known detector, such as Harris or DoG, and implement the interest point strength function in `corner_function` of `interest_point.py`. Run the code below to visualise your corner function output. Next detect corners as local maxima of this function by filling in `find_local_maxima` in the same file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute corner strength function\n",
    "\"\"\"\n",
    "print('[ Compute corner strength ]')\n",
    "ip_ex = interest_point.InterestPointExtractor()\n",
    "ip_fun = ip_ex.corner_function(img)\n",
    "\n",
    "# normalise for display\n",
    "[mn,mx]=np.percentile(ip_fun,[5,95])\n",
    "small_val=1e-9\n",
    "ip_fun_norm=(ip_fun-mn)/(mx-mn+small_val)\n",
    "ip_fun_norm=np.maximum(np.minimum(ip_fun_norm,1.0),0.0)\n",
    "\n",
    "\"\"\"\n",
    "Find local maxima of corner strength\n",
    "\"\"\"\n",
    "print('[ Find local maxima ]')\n",
    "row, col = ip_ex.find_local_maxima(ip_fun)\n",
    "ip = np.stack((row,col))\n",
    "\n",
    "ax1,ax2=im_util.plot_two_images(im_util.grey_to_rgb(ip_fun_norm),im)\n",
    "interest_point.draw_interest_points_ax(ip, ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptors and Matching [25%]\n",
    "\n",
    "Now let's match our interest points. Start by extracting a very simple descriptor that is simply a patch of pixels around the interest point. To do this, fill in the function `get_descriptors` in `interest_point.py`. The following code outputs a random sample of normalised descriptor patches. Check that the output looks sensible. Once you have this working, try varying the sample spacing in your descriptor patch. What problem exists with sample spacings > 1 pixel? How can this be fixed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract descriptors\n",
    "\"\"\"\n",
    "print('[ Extract descriptors ]')\n",
    "desc_ex=interest_point.DescriptorExtractor()\n",
    "descriptors=desc_ex.get_descriptors(img, ip)\n",
    "interest_point.plot_descriptors(descriptors,plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now match descriptors between a pair of images. Run the following two code blocks to extract your interest points and extract and match descriptors. The second code block calls a function to perform nearest-neighbour matching of descriptors and filtering using a ratio test. Take a look at the code and check you understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read a pair of input images and extract interest points\n",
    "\"\"\"\n",
    "image_dir='data/test'\n",
    "#im_filename1=image_dir+'/100-0023_img.jpg'\n",
    "#im_filename2=image_dir+'/100-0024_img.jpg'\n",
    "im_filename1=image_dir+'/100-0038_img.jpg'\n",
    "im_filename2=image_dir+'/100-0039_img.jpg'\n",
    "\n",
    "im1 = im_util.image_open(im_filename1)\n",
    "im2 = im_util.image_open(im_filename2)\n",
    "\n",
    "img1 = np.mean(im1, 2, keepdims=True)\n",
    "img2 = np.mean(im2, 2, keepdims=True)\n",
    "\n",
    "print('[ find interest points ]')\n",
    "t0=time()\n",
    "ip_ex = interest_point.InterestPointExtractor()\n",
    "ip1 = ip_ex.find_interest_points(img1)\n",
    "print(' found '+str(ip1.shape[1])+' in image 1')\n",
    "ip2 = ip_ex.find_interest_points(img2)\n",
    "print(' found '+str(ip2.shape[1])+' in image 2')\n",
    "t1=time()\n",
    "print(' % .2f secs ' % (t1-t0))\n",
    "\n",
    "print('[ drawing interest points ]')\n",
    "ax1,ax2=im_util.plot_two_images(im1,im2)\n",
    "t0=time()\n",
    "interest_point.draw_interest_points_ax(ip1, ax1)\n",
    "interest_point.draw_interest_points_ax(ip2, ax2)\n",
    "t1=time()\n",
    "print(' % .2f secs ' % (t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract and match descriptors\n",
    "\"\"\"\n",
    "print('[ extract descriptors ]')\n",
    "t0=time()\n",
    "desc_ex = interest_point.DescriptorExtractor()\n",
    "desc1 = desc_ex.get_descriptors(img1, ip1)\n",
    "desc2 = desc_ex.get_descriptors(img2, ip2)\n",
    "t1=time()\n",
    "print(' % .2f secs' % (t1-t0))\n",
    "\n",
    "print('[ match descriptors ]')\n",
    "match_idx,ratio_pass=desc_ex.match_ratio_test(desc1, desc2)\n",
    "num_ratio_pass=np.sum(ratio_pass)\n",
    "\n",
    "ipm=ip2[:,match_idx]\n",
    "\n",
    "ip1r=ip1[:,ratio_pass]\n",
    "ip2r=ipm[:,ratio_pass]\n",
    "\n",
    "N1,num_dims=desc1.shape\n",
    "print(' Number of interest points = '+str(N1))\n",
    "print(' Number of matches passing ratio test = '+str(num_ratio_pass))\n",
    "\n",
    "ax1,ax2=im_util.plot_two_images(im1,im2)\n",
    "interest_point.draw_matches_ax(ip1r, ip2r, ax1, ax2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code visualises matched descriptor patches. Can you distinguish the correct and incorrect matches? (reload to get another random sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot descriptors for matched points\n",
    "\"\"\"\n",
    "interest_point.plot_matching_descriptors(desc1,desc2,np.arange(0,ip1.shape[1]),match_idx,plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Improving Feature Matching [25%]\n",
    "\n",
    "Try varying the `ratio_threshold` parameter in the descriptor matcher (`DescriptorExtractor` class params). What are good settings for this parameter? If everything is working, you should see a good set of correctly matched points (aim for about 100 or more). Experiment with your interest point and descriptor implementations to find which parameters are important and try to get a good set of matches. Try out a new idea of your own to improve interest points or descriptors, and record your findings in the notebook below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO experiments with your detector/descriptors\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
