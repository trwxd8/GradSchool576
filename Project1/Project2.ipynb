{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Panoramic Image Stitching\n",
    "\n",
    "This is Project 2 for [UW CSE P576 Computer Vision](https://courses.cs.washington.edu/courses/csep576/18sp). \n",
    "\n",
    "**Getting Started:** You should complete **[Project 1](https://courses.cs.washington.edu/courses/csep57\n",
    "6/18sp/projects/Project1.html \"Project 1\")** first (you will need interest points and descriptors from this project). The source files for both projects are [here](https://courses.cs.washington.edu/courses/csep576/18sp/projects/project12/project12.zip \"Project 1 and 2 Source Files\"). To run the project locally you will need IPython/Jupyter installed, e.g., see instructions at http://jupyter.org/install.html. Launch Jupyter and open `Project2.ipynb`. Alternatively, you can import the standalone version of the notebook into [Colaboratory](https://colab.research.google.com \"Colab\") and run it without installing anything. Use File->Upload Notebook in Colab and open the notebook in `standalone/Project2s.ipynb`.\n",
    "\n",
    "**This project:** In this project you will implement a panoramic image stitcher. This will build on the interest points and descriptors developed in Project 1. You'll begin with geometric filtering via RANSAC, then estimate pairwise rotations and chain these together to align the panorama. When you have a basic stitcher working, improve it with better alignment, blending, or other new features and document your findings.\n",
    "\n",
    "**What to turn in:** Turn in a pdf or static html copy of your completed ipynb notebook as well as the source .ipynb and any source .py files that you modified. Clearly describe any enhancements or experiments you tried in your ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import os.path\n",
    "from time import time\n",
    "import types\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import im_util\n",
    "import interest_point\n",
    "import ransac\n",
    "import geometry\n",
    "import render\n",
    "import panorama\n",
    "\n",
    "%matplotlib inline\n",
    "# edit this line to change the figure size\n",
    "plt.rcParams['figure.figsize'] = (16.0, 10.0)\n",
    "# force auto-reload of import modules before running code \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Warping Test\n",
    "\n",
    "The code below warps an image using a 3x3 transformation matrix. Experiment with the matrix P to test some of the different 2D transformations described in class, e.g., similarity, affine and projective transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image\n",
    "image_dir='data/test'\n",
    "im_filename1=image_dir+'/100-0038_img.jpg'\n",
    "im=im_util.image_open(im_filename1)\n",
    "im_rows,im_cols,_=im.shape\n",
    "\n",
    "# set transformation matrix\n",
    "P=[[1, 0.2, -64],\n",
    "  [ 0, 1.1, -120],\n",
    "  [ 0, 5.2e-4, 0.83]]\n",
    "\n",
    "# warp coordinates\n",
    "r0,r1=-im_rows/2, im_rows*3/2\n",
    "c0,c1=-im_cols/2, im_cols*3/2\n",
    "warp_rows, warp_cols=im_rows, im_cols\n",
    "\n",
    "coords=im_util.coordinate_image(warp_rows,warp_cols,r0,r1,c0,c1)\n",
    "coords_t=im_util.transform_coordinates(coords, P)\n",
    "\n",
    "# visualise result\n",
    "warp_im1=im_util.warp_image(im,coords)\n",
    "warp_im2=im_util.warp_image(im,coords_t)\n",
    "alpha=im_util.warp_image(np.ones((im_rows,im_cols,1)),coords_t)\n",
    "result_im=warp_im2*alpha + 0.5*warp_im1*(1-alpha)\n",
    "\n",
    "ax1=plt.subplot(1,1,1)\n",
    "plt.imshow(result_im)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interest Points Test\n",
    "\n",
    "We will use the interest points and descriptors implemented in [Project 1](https://courses.cs.washington.edu/courses/csep576/18sp/projects/Project1.html \"Project 1\"). If you had trouble getting these to work, contact your TA.\n",
    "\n",
    "Run the two code blocks below to check your interest points and descriptors are working. For subsequent steps to run well, you should aim for about 100-1000 interest points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read a pair of input images and convert to grey\n",
    "\"\"\"\n",
    "image_dir='data/test'\n",
    "#im_filename1=image_dir+'/100-0023_img.jpg'\n",
    "#im_filename2=image_dir+'/100-0024_img.jpg'\n",
    "im_filename1=image_dir+'/100-0038_img.jpg'\n",
    "im_filename2=image_dir+'/100-0039_img.jpg'\n",
    "\n",
    "im1 = im_util.image_open(im_filename1)\n",
    "im2 = im_util.image_open(im_filename2)\n",
    "\n",
    "img1 = np.mean(im1, 2, keepdims=True)\n",
    "img2 = np.mean(im2, 2, keepdims=True)\n",
    "\n",
    "#optionally plot images\n",
    "#ax1,ax2=im_util.plot_two_images(im1, im2)\n",
    "\n",
    "\"\"\"\n",
    "Find interest points in the image pair\n",
    "\"\"\"\n",
    "print('[ find interest points ]')\n",
    "t0=time()\n",
    "ip_ex = interest_point.InterestPointExtractor()\n",
    "ip1 = ip_ex.find_interest_points(img1)\n",
    "print(' found '+str(ip1.shape[1])+' in image 1')\n",
    "ip2 = ip_ex.find_interest_points(img2)\n",
    "print(' found '+str(ip2.shape[1])+' in image 2')\n",
    "t1=time()\n",
    "print(' % .2f secs ' % (t1-t0))\n",
    "\n",
    "# optionally draw interest points\n",
    "#print('[ drawing interest points ]')\n",
    "#ax1,ax2=im_util.plot_two_images(im1,im2)\n",
    "#interest_point.draw_interest_points_ax(ip1, ax1)\n",
    "#interest_point.draw_interest_points_ax(ip2, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract and match descriptors\n",
    "\"\"\"\n",
    "print('[ extract descriptors ]')\n",
    "t0=time()\n",
    "desc_ex = interest_point.DescriptorExtractor()\n",
    "desc1 = desc_ex.get_descriptors(img1, ip1)\n",
    "desc2 = desc_ex.get_descriptors(img2, ip2)\n",
    "t1=time()\n",
    "print(' % .2f secs' % (t1-t0))\n",
    "\n",
    "print('[ match descriptors ]')\n",
    "t0=time()\n",
    "match_idx = desc_ex.match_descriptors(desc1, desc2)\n",
    "t1=time()\n",
    "print(' % .2f secs' % (t1-t0))\n",
    "\n",
    "ipm=ip2[:,match_idx]\n",
    "\n",
    "print('[ drawing matches ]')\n",
    "t0=time()\n",
    "ax1,ax2=im_util.plot_two_images(im1,im2)\n",
    "interest_point.draw_matches_ax(ip1, ipm, ax1, ax2)\n",
    "t1=time()\n",
    "print(' % .2f secs' % (t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANSAC Implementation\n",
    "\n",
    "We will now use RANSAC to find consistent matches.\n",
    "\n",
    "First we will implement a test to count the number of matches consistent with a Similarity transform. The code below generates a random Similarity transform S and a random set of points x. It then transforms the points and adds noise, and checks to see how many of these points are consistent with the ground truth transformation S.\n",
    "\n",
    "Open `ransac.py` and implement the function `consistent`. You should find a high fraction (~80% or more) points are consistent with the true Similarity transform S when running the code below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test RANSAC functions using synthetic data\n",
    "\"\"\"\n",
    "# make a random S matrix\n",
    "sd_pos=100\n",
    "sd_angle=np.pi\n",
    "theta=np.random.randn()*sd_angle\n",
    "tx=np.random.randn()*sd_pos\n",
    "ty=np.random.randn()*sd_pos\n",
    "ct=np.cos(theta)\n",
    "st=np.sin(theta)\n",
    "S=[[ct,st,tx],[-st,ct,ty],[0,0,1]]\n",
    "\n",
    "# generate random points\n",
    "num_points=100\n",
    "sd_points=20\n",
    "x = np.random.randn(2,num_points)*sd_points\n",
    "xh = geometry.hom(x)\n",
    "\n",
    "# transform points and add noise\n",
    "sd_noise=5\n",
    "yh = np.dot(S, xh)\n",
    "y = geometry.unhom(yh)\n",
    "yn = y + np.random.randn(2,num_points)*sd_noise\n",
    "\n",
    "print('[ Test of consistent ]')\n",
    "rn = ransac.RANSAC()\n",
    "inliers0=rn.consistent(S,x,yn)\n",
    "num_consistent=np.sum(inliers0)\n",
    "print(' number of points consistent with true S = '+str(num_consistent))\n",
    "if (num_consistent > 0.75*num_points):\n",
    "    print(' consistency check is working!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now select a sample of 2 point corresondences and compute the Similarity transform corresponding to this pair. Implement `compute_similarity` in `ransac.py` and run the code below to compute the number of inliers. Try varying the indices of the sample to see how the number of inliers varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[ Test of compute_similarity ]')\n",
    "sample=[0,1]\n",
    "S1=rn.compute_similarity(x[:,sample],yn[:,sample])\n",
    "inliers1=rn.consistent(S1,x,yn)\n",
    "num_consistent=np.sum(inliers1)\n",
    "print(' number of points consistent with sample S = '+str(num_consistent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, finish the implementation of RANSAC by completing `ransac_similarity` in `ransac.py`. When completed you should find most of the points are labelled consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[ Test of ransac_similarity ]')\n",
    "S2, inliers2=rn.ransac_similarity(x, yn)\n",
    "num_consistent=np.sum(inliers2)\n",
    "print(' number of points consistent with ransac S = '+str(num_consistent))\n",
    "if (num_consistent > 0.75*num_points):\n",
    "    print(' ransac succeeded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now move away from our synthetic test data and run the same code on the interest point matches obtained using the input image pair above. Review the code below and check that the output looks reasonable. You should see a good set of geometrically consistent matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Perform RANSAC on interest point matches\n",
    "\"\"\"\n",
    "print('[ do ransac ]')\n",
    "t0=time()\n",
    "rn = ransac.RANSAC()\n",
    "S, inliers = rn.ransac_similarity(ip1,ipm)\n",
    "t1=time()\n",
    "num_inliers_s = np.sum(inliers)\n",
    "print(' found '+str(num_inliers_s)+' matches')\n",
    "print(' % .2f secs' % (t1-t0))\n",
    "\n",
    "ip1c = ip1[:, inliers]\n",
    "ipmc = ipm[:, inliers]\n",
    "\n",
    "print('[ drawing matches ]')\n",
    "t0=time()\n",
    "ax1,ax2=im_util.plot_two_images(im1,im2)\n",
    "interest_point.draw_matches_ax(ip1c, ipmc, ax1, ax2)\n",
    "t1=time()\n",
    "print(' % .2f secs' % (t1-t0))\n",
    "\n",
    "# optionally plot descriptors for matched points\n",
    "#inlier_id=np.flatnonzero(inliers)\n",
    "#match_id=match_idx[inlier_id]\n",
    "#interest_point.plot_matching_descriptors(desc1,desc2,inlier_id,match_id,plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotation Estimation\n",
    "\n",
    "The next task is to estimate the true rotation between the images. To do this, we'll take a guess at the field of view of our input images, and use a closed form algorithm to estimate the rotation. Open `geometry.py` and complete the implementation of `compute_rotation`. You should find that a large number of the matches are consistent with your rotation, and the pairwise warped images should look sensible. Try experimenting with the field of view parameter. What is the best field of view for these images? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Estimate rotation matrix by least squares\n",
    "\"\"\"\n",
    "print('[ estimate rotation ]')\n",
    "t0=time()\n",
    "# Note: assume field of view of 45 degrees\n",
    "fov_degrees=45\n",
    "print(' assuming fov='+str(fov_degrees))\n",
    "K1 = geometry.get_calibration(im1.shape, fov_degrees)\n",
    "K2 = geometry.get_calibration(im2.shape, fov_degrees)\n",
    "R,H = geometry.compute_rotation(ip1c, ipmc, K1, K2)\n",
    "\n",
    "num_inliers_r = np.sum(rn.consistent(H, ip1, ipm))\n",
    "print(' num consistent with rotation = '+str(num_inliers_r))\n",
    "if (num_inliers_r>0.9 * num_inliers_s):\n",
    "    print(' compute rotation succeeded!')\n",
    "t1=time()\n",
    "print(' % .2f secs' % (t1-t0))\n",
    "    \n",
    "print('[ test pairwise warp ]')\n",
    "t0=time()\n",
    "im1_w, im2_w = render.pairwise_warp(im1, im2, H)\n",
    "_= im_util.plot_two_images(0.5*(im1+im2_w), 0.5*(im2+im1_w))\n",
    "t1=time()\n",
    "print(' % .2f secs' % (t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code renders the aligned images in a spherical coordinate system. Check that the images are well aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Render 2 images in spherical coordinates\n",
    "\"\"\"\n",
    "images=[im1,im2]\n",
    "P1=K1\n",
    "P2=np.dot(K2,R)\n",
    "P_matrices=[P1,P2]\n",
    "\n",
    "render_params={}\n",
    "render_params['render_width']=800\n",
    "render_params['theta_min']=-45\n",
    "render_params['theta_max']=45\n",
    "render_params['phi_min']=-30\n",
    "render_params['phi_max']=30\n",
    "\n",
    "print ('[ render aligned images ]')\n",
    "t0=time()\n",
    "pano_im=render.render_spherical(images, P_matrices, render_params)\n",
    "t1=time()\n",
    "print(' % .2f secs' % (t1-t0))\n",
    "\n",
    "plt.plot()\n",
    "plt.imshow(pano_im)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add more images! The method `PanormaStitcher` class in `panorama.py` takes a set of images as input and wraps the interest point and matching code in the method `match_images`. Take a look at this function and test it on a set of images using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read a set of input images\n",
    "\"\"\"\n",
    "\n",
    "print('[ read images ]')\n",
    "image_dir='data/test'\n",
    "im_filenames=os.listdir(image_dir)\n",
    "im_filenames=[image_dir+'/'+fname for fname in im_filenames]\n",
    "\n",
    "#im_filenames=[]\n",
    "#im_filenames.append(image_dir+'/100-0023_img.jpg')\n",
    "#im_filenames.append(image_dir+'/100-0024_img.jpg')\n",
    "#im_filenames.append(image_dir+'/100-0038_img.jpg')\n",
    "#im_filenames.append(image_dir+'/100-0039_img.jpg')\n",
    "\n",
    "images=[]\n",
    "for fname in im_filenames:\n",
    "  images.append(im_util.image_open(fname))\n",
    "\n",
    "\"\"\"\n",
    "Stitch images\n",
    "\"\"\"\n",
    "stitch_params={}\n",
    "stitch_params['fov_degrees']=45\n",
    "\n",
    "num_images = len(im_filenames)\n",
    "print(' stitching '+str(num_images)+' images')\n",
    "\n",
    "pano=panorama.PanoramaStitcher(images, stitch_params)\n",
    "pano.match_images()\n",
    "\n",
    "print(' num_matches=')\n",
    "print(pano.num_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write code to compute a rotation matrix for each image (the first image is assumed to be the identity rotation) by chaining together pairwise rotations. The code for this should go in `align_panorama` in `panorama.py`.\n",
    "\n",
    "You can now use the `render` method to stich all images in spherical coordinates, as shown in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pano.align_panorama()\n",
    "\n",
    "render_params={}\n",
    "render_params['render_width']=800\n",
    "render_params['theta_min']=-45\n",
    "render_params['theta_max']=45\n",
    "render_params['phi_min']=-30\n",
    "render_params['phi_max']=30\n",
    "\n",
    "pano_im = pano.render(render_params)\n",
    "\n",
    "plt.plot()\n",
    "plt.imshow(pano_im)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Improving the Panorama Stitcher\n",
    "\n",
    "You should now have a complete implementation of a basic panorama stitcher. Try it out using a few different image sets and make a note of any issues/artifacts in the results. How could the results be improved? Write a list of possible improvements, and think of new features you might like to add. Now implement some of these improvements / new features and document your work in the notebook below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO your improvements to the panorama stitcher"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
